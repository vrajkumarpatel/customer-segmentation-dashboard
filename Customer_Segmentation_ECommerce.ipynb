{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-Commerce Customer Segmentation Dashboard\n",
    "**Author:** Vrajkumar Patel  ",
    "**Date:** 2025-11-23\n",
    "\n",
    "This portfolio-ready notebook performs RFM analysis and K-Means clustering on e-commerce transactions to identify high-value, loyal, and at-risk customers. It includes data cleaning, feature engineering, EDA, clustering, and actionable insights with clear explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Segmentation for E-Commerce\n",
    "\n",
    "End-to-end, portfolio-ready analysis that segments customers by purchasing behavior using RFM and K-Means. The notebook automates data loading, cleaning, feature engineering, EDA, clustering, and actionable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup\n",
    "Imports and configuration for reproducibility and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
 "source": [
    "# Setup: imports and environment configuration\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import math\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "try:\n",
    "    import plotly.express as px\n",
    "except Exception:\n",
    "    px = None\n",
    "\n",
    "np.random.seed(42)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.2f}')\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "\n",
    "print('Environment ready:', {'python': sys.version.split()[0], 'pandas': pd.__version__})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Import\n",
    "Load a publicly available e-commerce dataset. This notebook uses the UCI \"Online Retail\" dataset (Excel) which includes: `InvoiceNo`, `CustomerID`, `Description` (Product), `Quantity`, `UnitPrice` (Price), and `InvoiceDate` (Date). If remote loading fails, synthetic data is generated to keep the notebook reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
 "source": [
    "# Step 1: Data import from UCI or synthetic fallback\n",
    "DATA_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx'\n",
    "\n",
    "def load_ecommerce_data():\n",
    "    df = None\n",
    "    try:\n",
    "        df = pd.read_excel(DATA_URL, dtype={'InvoiceNo': str, 'StockCode': str})\n",
    "        print('Loaded dataset from UCI:', DATA_URL)\n",
    "    except Exception as e:\n",
    "        print('Remote load failed, generating synthetic dataset')\n",
    "        n_customers = 500\n",
    "        n_invoices = 3000\n",
    "        start_date = dt.datetime(2010, 12, 1)\n",
    "        end_date = dt.datetime(2011, 12, 9)\n",
    "        date_range = pd.date_range(start_date, end_date, freq='H')\n",
    "        cust_ids = np.random.choice(range(10000, 10000 + n_customers), n_invoices)\n",
    "        invoice_ids = [str(500000 + i) for i in range(n_invoices)]\n",
    "        quantities = np.random.randint(1, 10, size=n_invoices)\n",
    "        prices = np.round(np.random.uniform(1.0, 50.0, size=n_invoices), 2)\n",
    "        products = np.random.choice(['Widget A', 'Widget B', 'Gadget C', 'Accessory D'], size=n_invoices)\n",
    "        dates = np.random.choice(date_range, size=n_invoices)\n",
    "        df = pd.DataFrame({\n",
    "            'InvoiceNo': invoice_ids,\n",
    "            'CustomerID': cust_ids,\n",
    "            'Description': products,\n",
    "            'Quantity': quantities,\n",
    "            'UnitPrice': prices,\n",
    "            'InvoiceDate': dates\n",
    "        })\n",
    "    df['Invoice ID'] = df['InvoiceNo']\n",
    "    df['Customer ID'] = df['CustomerID']\n",
    "    df['Product'] = df['Description']\n",
    "    df['Price'] = df['UnitPrice']\n",
    "    df['Date'] = pd.to_datetime(df['InvoiceDate'])\n",
    "    return df\n",
    "\n",
    "raw_df = load_ecommerce_data()\n",
    "display(raw_df.head())\n",
    "print('Shape:', raw_df.shape)\n",
    "buffer = io.StringIO()\n",
    "raw_df.info(buf=buffer)\n",
    "print(buffer.getvalue())\n",
    "display(raw_df.describe(include='all').transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning\n",
    "- Handle missing values.\n",
    "- Remove duplicates.\n",
    "- Correct inconsistent data (negative or zero quantities/prices).\n",
    "- Convert date columns to datetime.\n",
    "- Aggregate fields needed for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
 "source": [
    "# Step 2: Clean transactions, compute totals, standardize columns\n",
    "def clean_data(df):\n",
    "    before_rows = len(df)\n",
    "    df = df.copy()\n",
    "    df = df.dropna(subset=['CustomerID'])\n",
    "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
    "    df = df.dropna(subset=['InvoiceDate'])\n",
    "    df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
    "    df = df.drop_duplicates()\n",
    "    df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
    "    df['Invoice ID'] = df['InvoiceNo']\n",
    "    df['Customer ID'] = df['CustomerID'].astype(int)\n",
    "    df['Product'] = df['Description']\n",
    "    df['Price'] = df['UnitPrice']\n",
    "    df['Date'] = df['InvoiceDate']\n",
    "    after_rows = len(df)\n",
    "    print('Rows before:', before_rows, 'after:', after_rows)\n",
    "    return df\n",
    "\n",
    "clean_df = clean_data(raw_df)\n",
    "display(clean_df.head())\n",
    "print('Unique customers:', clean_df['Customer ID'].nunique())\n",
    "print('Distinct invoices:', clean_df['Invoice ID'].nunique())"
   ]
  },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
    "## Step 3: Feature Engineering (RFM)\n",
    "Compute Recency (days since last purchase), Frequency (number of invoices), Monetary (total spend), optional features such as average order value and average days between purchases, plus product preferences and seasonality signals."
  ]
 },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
 "source": [
    "# Step 3: Compute customer-level RFM and seasonality features\n",
    "def compute_rfm(df):\n",
    "    snapshot_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
    "    recency = df.groupby('Customer ID')['InvoiceDate'].max().apply(lambda x: (snapshot_date - x).days)\n",
    "    frequency = df.groupby('Customer ID')['Invoice ID'].nunique()\n",
    "    monetary = df.groupby('Customer ID')['TotalPrice'].sum()\n",
    "    aov = monetary / frequency\n",
    "    def avg_days_between(group):\n",
    "        dates = group['InvoiceDate'].drop_duplicates().sort_values()\n",
    "        if len(dates) < 2:\n",
    "            return np.nan\n",
    "        deltas = dates.diff().dropna().dt.days\n",
    "        return deltas.mean()\n",
    "    days_between = df.groupby('Customer ID').apply(avg_days_between)\n",
    "    month_series = df['InvoiceDate'].dt.month\n",
    "    quarter_series = df['InvoiceDate'].dt.quarter\n",
    "    weekend_series = df['InvoiceDate'].dt.weekday >= 5\n",
    "    top_product = df.groupby('Customer ID')['Description'].agg(lambda s: s.value_counts().idxmax())\n",
    "    peak_month = df.assign(Month=month_series).groupby('Customer ID')['Month'].agg(lambda s: s.value_counts().idxmax())\n",
    "    q4_share = df.assign(Q=quarter_series).groupby('Customer ID')['Q'].apply(lambda s: (s == 4).mean())\n",
    "    weekend_ratio = df.assign(Weekend=weekend_series).groupby('Customer ID')['Weekend'].mean()\n",
    "    rfm = pd.DataFrame({\n",
    "        'Recency': recency,\n",
    "        'Frequency': frequency,\n",
    "        'Monetary': monetary,\n",
    "        'AvgOrderValue': aov,\n",
    "        'DaysBetweenPurchases': days_between,\n",
    "        'TopProduct': top_product,\n",
    "        'PeakMonth': peak_month,\n",
    "        'Q4Share': q4_share,\n",
    "        'WeekendRatio': weekend_ratio\n",
    "    })\n",
    "    rfm = rfm.fillna({'DaysBetweenPurchases': rfm['DaysBetweenPurchases'].median()})\n",
    "    rfm = rfm.sort_values(by=['Monetary'], ascending=False)\n",
    "    print('RFM computed for customers:', len(rfm))\n",
    "    return rfm\n",
    "\n",
    "rfm = compute_rfm(clean_df)\n",
    "display(rfm.head())\n",
    "display(rfm.describe().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Exploratory Data Analysis (EDA)\n",
    "Visualize distributions, relationships, and correlations among RFM features to identify patterns and customer groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
 "source": [
    "# Step 4: EDA visuals for distributions, relationships, correlations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 9))\n",
    "sns.histplot(rfm['Recency'], kde=True, ax=axes[0,0], color='steelblue')\n",
    "axes[0,0].set_title('Recency (days)')\n",
    "sns.histplot(rfm['Frequency'], kde=True, ax=axes[0,1], color='seagreen')\n",
    "axes[0,1].set_title('Frequency (invoices)')\n",
    "sns.histplot(rfm['Monetary'], kde=True, ax=axes[0,2], color='salmon')\n",
    "axes[0,2].set_title('Monetary (total spend)')\n",
    "sns.boxplot(y=rfm['Recency'], ax=axes[1,0], color='steelblue')\n",
    "axes[1,0].set_title('Recency Boxplot')\n",
    "sns.boxplot(y=rfm['Frequency'], ax=axes[1,1], color='seagreen')\n",
    "axes[1,1].set_title('Frequency Boxplot')\n",
    "sns.boxplot(y=rfm['Monetary'], ax=axes[1,2], color='salmon')\n",
    "axes[1,2].set_title('Monetary Boxplot')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.scatterplot(x='Recency', y='Frequency', data=rfm, alpha=0.6)\n",
    "plt.title('Recency vs Frequency')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.scatterplot(x='Frequency', y='Monetary', data=rfm, alpha=0.6)\n",
    "plt.title('Frequency vs Monetary')\n",
    "plt.show()\n",
    "\n",
    "corr = rfm[['Recency','Frequency','Monetary','AvgOrderValue','DaysBetweenPurchases','Q4Share','WeekendRatio']].corr()\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.heatmap(corr, annot=True, cmap='vlag', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "display(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Customer Segmentation (Clustering)\n",
    "Use K-Means on standardized RFM features. Determine optimal clusters via elbow (inertia) and silhouette scores, then analyze cluster characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
 "source": [
    "# Step 5: K-Means clustering on standardized RFM with model selection\n",
    "features = rfm[['Recency','Frequency','Monetary']].copy()\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(features)\n",
    "ks = list(range(2, 9))\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "for k in ks:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(X)\n",
    "    inertias.append(km.inertia_)\n",
    "    labels = km.labels_\n",
    "    score = silhouette_score(X, labels)\n",
    "    silhouettes.append(score)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.title('Elbow: Inertia vs k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(ks, silhouettes, '-o', color='orange')\n",
    "plt.title('Silhouette vs k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_k = ks[int(np.argmax(silhouettes))]\n",
    "print('Chosen k by silhouette:', best_k)\n",
    "km_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "rfm['Cluster'] = km_final.fit_predict(X)\n",
    "cluster_stats = rfm.groupby('Cluster').agg({\n",
    "    'Recency':'mean',\n",
    "    'Frequency':'mean',\n",
    "    'Monetary':'mean',\n",
    "    'AvgOrderValue':'mean',\n",
    "    'DaysBetweenPurchases':'mean'\n",
    "}).round(2)\n",
    "cluster_stats['Size'] = rfm.groupby('Cluster').size()\n",
    "display(cluster_stats)\n",
    "rfm_clusters = rfm.copy()\n",
    "rfm_clusters.reset_index(inplace=True)\n",
    "display(rfm_clusters.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualization & Dashboard\n",
    "Visualize clusters in 2D and summarize cluster characteristics in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
 "source": [
    "# Step 6: 2D cluster visualization and summary table\n",
    "if px is not None:\n",
    "    fig = px.scatter(rfm, x='Frequency', y='Monetary', color='Cluster', title='Clusters: Frequency vs Monetary',\n",
    "                     labels={'Frequency':'Frequency','Monetary':'Monetary'})\n",
    "    fig.show()\n",
    "    fig2 = px.scatter(rfm, x='Recency', y='Frequency', color='Cluster', title='Clusters: Recency vs Frequency',\n",
    "                      labels={'Recency':'Recency','Frequency':'Frequency'})\n",
    "    fig2.show()\n",
    "else:\n",
    "    plt.figure(figsize=(7,5))\n",
    "    sns.scatterplot(data=rfm, x='Frequency', y='Monetary', hue='Cluster', palette='tab10', alpha=0.7)\n",
    "    plt.title('Clusters: Frequency vs Monetary')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(7,5))\n",
    "    sns.scatterplot(data=rfm, x='Recency', y='Frequency', hue='Cluster', palette='tab10', alpha=0.7)\n",
    "    plt.title('Clusters: Recency vs Frequency')\n",
    "    plt.show()\n",
    "\n",
    "display(cluster_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Insights & Recommendations\n",
    "Interpret clusters and propose actions such as loyalty programs, re-engagement campaigns, and targeted marketing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Interpretation\n",
    "- High-value: Low Recency, High Frequency, High Monetary. Recommend exclusive loyalty perks, early access, bundled offers.\n",
    "- Loyal: Moderate Recency, High Frequency, Moderate Monetary. Recommend points-based rewards, referral incentives, personalized recommendations.\n",
    "- At-risk: High Recency, Low Frequency, Moderate/Low Monetary. Recommend win-back emails, time-limited discounts, reactivation nudges.\n",
    "- Low-value: Variable Recency, Low Frequency, Low Monetary. Recommend awareness campaigns, cross-sell essentials, optimize onboarding.\n",
    "\n",
    "Cluster-specific actions should align with product preferences observed from `Description` and seasonal patterns in `InvoiceDate`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8a: Persist Artifacts\n",
    "Save clustered customers and cluster stats as CSV for downstream workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
 "source": [
    "# Step 8a: Persist artifacts for dashboard and downstream workflows\n",
    "out_dir = 'data'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "rfm_clusters_df.to_csv(os.path.join(out_dir, 'rfm_clusters.csv'), index=False)\n",
    "rfm_cluster_stats.to_csv(os.path.join(out_dir, 'rfm_cluster_stats.csv'))\n",
    "print('Saved:', os.path.join(out_dir, 'rfm_clusters.csv'))\n",
    "print('Saved:', os.path.join(out_dir, 'rfm_cluster_stats.csv'))"
   ]
  },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
    "## Step 8: End-to-End Automation\n",
    "Pipeline function to run the full workflow and return clustered RFM data with stats for reuse and deployment."
  ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
 "source": [
    "# Step 8: End-to-end pipeline to compute clusters and stats\n",
    "def run_pipeline():\n",
    "    d0 = load_ecommerce_data()\n",
    "    d1 = clean_data(d0)\n",
    "    r = compute_rfm(d1)\n",
    "    feats = r[['Recency','Frequency','Monetary']]\n",
    "    sc = StandardScaler()\n",
    "    X_ = sc.fit_transform(feats)\n",
    "    ks_ = list(range(2, 9))\n",
    "    sils_ = []\n",
    "    for k in ks_:\n",
    "        km_ = KMeans(n_clusters=k, random_state=42, n_init=10).fit(X_)\n",
    "        sils_.append(silhouette_score(X_, km_.labels_))\n",
    "    kstar = ks_[int(np.argmax(sils_))]\n",
    "    km_ = KMeans(n_clusters=kstar, random_state=42, n_init=10).fit(X_)\n",
    "    r['Cluster'] = km_.labels_\n",
    "    stats = r.groupby('Cluster').agg({\n",
    "        'Recency':'mean','Frequency':'mean','Monetary':'mean','AvgOrderValue':'mean','DaysBetweenPurchases':'mean'\n",
    "    }).round(2)\n",
    "    stats['Size'] = r.groupby('Cluster').size()\n",
    "    return r.reset_index(), stats, kstar\n",
    "\n",
    "rfm_clusters_df, rfm_cluster_stats, chosen_k = run_pipeline()\n",
    "print('Pipeline complete. Chosen k:', chosen_k)\n",
    "display(rfm_cluster_stats)\n",
    "display(rfm_clusters_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion & Recommendations\n",
    "- High-value segments (low R, high F/M) benefit from loyalty rewards and premium offers.\n",
    "- At-risk segments (high R) respond to win-back campaigns and time-limited discounts.\n",
    "- Product preferences (TopProduct) and seasonal signals (PeakMonth) enable tailored promotions.\n",
    "- This analysis supports targeted marketing, retention strategies, and revenue growth via segmentation."
   ]
  }
],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}